# TODO
    # Refactoring
        # In train.py
            # Before the actual loop/code, write pseudo-code for the loop
            # Before running the model, first check if the log dir can be made
        # In trainer.py + trainer_machine.py
            # Split train and eval functions. Let eval() return the threshold_checkpoints
        # Let each dataset class/ label picker/ trainer machine class write their own logging function
            # Maybe verbose version + simple version?
        # Combine ICALR and TrainerMachine file.
            # 1 - Read ICALR closely
                # Where it calls Network? expand those function
                # Split naive, proto, smooth to different class or remove their existence at all
                # Check the logging difference. See if Network class is most update to date.
    # Features
        # I removed log/save_first_round_model. We wanna provide a separate eval.py maybe.
        # Split train and eval? Like open set active learning
            # First train, then do active learning, then finetune and see the 
            # e.g. Rounds represent active learning round? Before every active learning round
            #      training will happen.
        # Save models at last checkpoint
        # Actually, let each subtype of trainer machine have their own save and load functions
            # Need to figure out when we want to call these save and load functions
            # e.g. number of discovered classes how to save and load
    # Bugs
        # Debug Openmax: The roc curve should be flipped after all the open set scores are negated.
                        We should in fact check this for all the algorithms.

# Open set active learning (Deep metric learning + ULDR unlabeled to labeled density)
    # NN:
        # Algorithm for fast approx NN?
        # Size of neighers?
        # Periodic async update is how many epochs?
    # Distance
        # Euclidean distance only?
    # Assumption
        # After training the network, novel class examples are indeed far away from known classes?
    # Implementation
        # Network VGG16, feature embedding of FC7 and size 4096
        # How is the shared guassian picked
        # Is it SGD?


# Metric
    # Precision v.s. Recall and ROC curve
        # This tutorial says that when data is imbalanced, we should use PR instead of ROC?
        #https://machinelearningmastery.com/roc-curves-and-precision-recall-curves-for-classification-in-python/
    # The open set active learning paper present F-1 and acc results. How are their threshold be picked?


# Implementation
    # Remove forward handle, use output of network
    # Save softmax network + cos network (by today)
    # Save c2AE, learning loss (later)


# New setup
    # train.py
        # Initialize the dataset. ï¼ˆOr load from an existing dataset)
            # Dataset should be able to be splitted into (discovered_classes, undiscovered_classes, open_classes)
            # This class split should be able to make arbitrarily. (Two options: Order or random)
            # We should have fix number of samples per class. The samples should be fixed. (One options: Random.)
            # Should be able to save and load the dataset split. Including class split, the initial samples per class.
        # Initialize a Trainer. The trainer must have these functions:
            # train(). Args: discovered_samples, discovered_classes.
                # Can also load from a pre-existing state_dict.
                # 3 versions: Softmax Network. Cosine network. Metric learning. (Maybe plus sigmoid network and learning loss)
            # active_query() Args: unlabeled_samples, budget. Returns: new queries and labels.
                # 5 algorithms:
                    # ULDR, Coreset, Entropy/Softmax, Learning Loss, 
            # finetune() Args: discovered_samples, discovered_classes.
                # different ways to finetune the model?
            # Each of the above 3 steps should save the checkpoint/state_dict for later use.
            # Then for evaluation:
            # eval_closed_set()
                # Report the overall accuracy on all closed set classes
            # eval_open_set()
                # should support different versions of open set algorithm:
                    # Softmax/Entropy, OpenMax, C2AE (need retrain), Nearest Neighbor, ULDR
                # Log the open set score/label for each test samples.
        

        # Finetune - should it be different from train?


# Check pairwise distance first. Validate gaussian kernel. 
# Run BIRD dataset with softmax network and ULDR/Entropy/Random Query. 

# Rerun for coreset/uldr, noralized cosine network. Drop sigma to 1.

# Check pairwise distance first. For softmax network/cosine network/cosine network with normalized.

# Average pairwise distance
    # For softmax network 
        # L2 distance
            # Unlabeled to Unlabeled is 117.06800079345703
            # Unlabeled to Labeled is 141.25936889648438
        # Cosine norm distance
            # Unlabeled to Unlabeled is 0.013295680284500122
            # Unlabeled to Labeled is 0.0050145648419857025
    # For cosine network   
        # L2 distance  
            # Unlabeled to Unlabeled is 19.5631103515625
            # Unlabeled to Labeled is 28.473447799682617
        # Cosine norm distance
            # Unlabeled to Unlabeled is 0.0062626698
            # Unlabeled to Labeled is 0.0011482930


# TODO
    # First softmax/cosine network - finetune step use a new network, lr 0.1 200 epochs, decay every 60 steps
    # Train from scratch for 40000 see the upperbound. See if regular setting actually hit the upperbound
    # Compute accurracy on trainset using the evaluation script for test set
    # Fix the color/marker on the graphs
    # Sort the labels based on the results

# Test accuracy when using all training examples
    # Few class, start with 8 classes 250 samples each
        # No finetuning - train a new network (200 epochs, lr 0.1, decay every 60 epochs): 77.81
        # With finetuning (140 epochs, lr 0.01, decay every 60 epochs): 68.59
        # With finetuning (200 epochs, lr 0.1, decay every 60 epochs): 76.8
    # Few samples, start with 40 classes 50 samples each  
        # No finetuning - 77.99
        # With finetuning (140 epochs, lr 0.01) - 68.33
        # With finetuning (200 epochs, lr 0.1) - 77.0
    # Regular, start with 40 classes 250 samples Each
        # No finetuning - 77.93
        # With finetuning - 74.90
    